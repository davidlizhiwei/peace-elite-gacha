#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸ç¾æ ¼é²è‚½ (Semaglutide) å®éªŒ Readout ç›‘æ§è„šæœ¬
å®šæœŸæ£€æŸ¥æœ€æ–°ä¸´åºŠè¯•éªŒç»“æœå¹¶å‘é€é’‰é’‰é€šçŸ¥
"""

import requests
import json
import hashlib
import os
from datetime import datetime

# é…ç½®
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=a28857b2fb6219f617702dda638035351329fd6dd4fdcc8ac875f4ff8fb698bf"
DATA_FILE = os.path.join(os.path.dirname(__file__), "semaglutide-cache.json")
SEARCH_QUERIES = [
    "å¸ç¾æ ¼é²è‚½ semaglutide ä¸´åºŠè¯•éªŒ readout 2026",
    "semaglutide clinical trial results 2026",
    "è¯ºå’Œè¯ºå¾· å¸ç¾æ ¼é²è‚½ æ–°è¯ è·æ‰¹ 2026",
    "Wegovy Ozempic FDA approval 2026",
]

# æœç´¢è„šæœ¬è·¯å¾„
SKILLS_ROOT = os.environ.get("SKILLS_ROOT", "/Users/davidli/Library/Application Support/LobsterAI/SKILLs")
SEARCH_SCRIPT = os.path.join(SKILLS_ROOT, "web-search/scripts/search-hybrid.sh")


def load_cache():
    """åŠ è½½å·²é€šçŸ¥çš„è®°å½•"""
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"notified_urls": [], "last_check": None}


def save_cache(data):
    """ä¿å­˜å·²é€šçŸ¥çš„è®°å½•"""
    data["last_check"] = datetime.now().isoformat()
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def run_search(query):
    """æ‰§è¡Œæœç´¢"""
    import subprocess
    try:
        result = subprocess.run(
            ["bash", SEARCH_SCRIPT, query, "10"],
            capture_output=True,
            text=True,
            timeout=60
        )
        return result.stdout
    except Exception as e:
        print(f"æœç´¢å¤±è´¥ï¼š{e}")
        return ""


def parse_search_results(output):
    """è§£ææœç´¢ç»“æœ"""
    results = []
    lines = output.split("\n")

    current_item = {}
    for line in lines:
        if line.startswith("## "):
            if current_item.get("title"):
                results.append(current_item)
            current_item = {"title": line[3:].strip()}
        elif line.startswith("**URL:**"):
            url = line.replace("**URL:**", "").strip()
            # æå–çº¯ URL
            if "[" in url and "]" in url:
                url = url.split("](")[-1].replace(")", "")
            current_item["url"] = url
        elif line.startswith("**Date:**"):
            current_item["date"] = line.replace("**Date:**", "").strip()

    if current_item.get("title"):
        results.append(current_item)

    return results


def send_dingtalk_notification(new_items):
    """å‘é€é’‰é’‰é€šçŸ¥"""
    if not new_items:
        return False

    # æ„å»ºæ¶ˆæ¯å†…å®¹
    text = "## ğŸ’Š å¸ç¾æ ¼é²è‚½å®éªŒ Readout æ›´æ–°\n\n"
    text += f"å‘ç° **{len(new_items)}** æ¡æ–°è¿›å±•ï¼š\n\n"

    for i, item in enumerate(new_items[:5], 1):  # æœ€å¤šæ˜¾ç¤º 5 æ¡
        text += f"{i}. **{item.get('title', 'æ— æ ‡é¢˜')}**\n"
        text += f"   ğŸ”— [æŸ¥çœ‹è¯¦æƒ…]({item.get('url', '#')})\n"
        if item.get('date'):
            text += f"   ğŸ“… {item['date']}\n\n"

    if len(new_items) > 5:
        text += f"\n... è¿˜æœ‰ {len(new_items) - 5} æ¡ï¼Œè¯·è®¿é—®å®Œæ•´æŠ¥å‘Š\n"

    text += "\n---\n_ç›‘æ§æ—¶é—´ï¼š" + datetime.now().strftime("%Y-%m-%d %H:%M") + "_"

    payload = {
        "msgtype": "markdown",
        "markdown": {
            "title": "å¸ç¾æ ¼é²è‚½å®éªŒ Readout æ›´æ–°",
            "text": text
        }
    }

    try:
        response = requests.post(
            DINGTALK_WEBHOOK,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        result = response.json()
        if result.get("errcode") == 0:
            print(f"âœ“ é’‰é’‰é€šçŸ¥å·²å‘é€ ({len(new_items)} æ¡)")
            return True
        else:
            print(f"âœ— é’‰é’‰é€šçŸ¥å¤±è´¥ï¼š{result}")
            return False
    except Exception as e:
        print(f"âœ— å‘é€é€šçŸ¥å¼‚å¸¸ï¼š{e}")
        return False


def check_updates():
    """æ£€æŸ¥æ›´æ–°"""
    print(f"[{datetime.now().isoformat()}] å¼€å§‹æ£€æŸ¥å¸ç¾æ ¼é²è‚½å®éªŒ readout...")

    cache = load_cache()
    notified_urls = set(cache.get("notified_urls", []))
    new_items = []

    for query in SEARCH_QUERIES:
        print(f"  æœç´¢ï¼š{query}")
        output = run_search(query)
        results = parse_search_results(output)

        for item in results:
            url = item.get("url", "")
            if url and url not in notified_urls:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„ï¼ˆURL ä¸åœ¨ç¼“å­˜ä¸­ï¼‰
                # ä¸”æ—¥æœŸæ˜¯è¿‘æœŸçš„ï¼ˆ2026 å¹´ï¼‰
                date_str = item.get("date", "")
                if "2026" in date_str or "hours ago" in date_str.lower() or "days ago" in date_str.lower():
                    item["query"] = query
                    new_items.append(item)
                    notified_urls.add(url)

    # å»é‡ï¼ˆæŒ‰ URLï¼‰
    seen_urls = set()
    unique_new_items = []
    for item in new_items:
        if item.get("url") not in seen_urls:
            seen_urls.add(item.get("url"))
            unique_new_items.append(item)

    print(f"  å‘ç° {len(unique_new_items)} æ¡æ–°å†…å®¹")

    if unique_new_items:
        send_dingtalk_notification(unique_new_items)

        # æ›´æ–°ç¼“å­˜
        cache["notified_urls"] = list(notified_urls)
        save_cache(cache)

    return len(unique_new_items)


if __name__ == "__main__":
    count = check_updates()
    print(f"å®Œæˆï¼æ–°å¢ï¼š{count} æ¡")
